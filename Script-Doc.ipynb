{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1542f8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc0c2e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting python_docx\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lxml>=2.3.2\n",
      "  Downloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: python_docx\n",
      "  Building wheel for python_docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python_docx: filename=python_docx-0.8.11-py3-none-any.whl size=184490 sha256=07d5d6f36867ab26f5556964c5be844786193250a5fbcb296b8f8ede37640ff0\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/65/e1/9b/0c38fe6cfe02a9fe31cb6b4efd90985f17354d7f77872f2def\n",
      "Successfully built python_docx\n",
      "Installing collected packages: lxml, python_docx\n",
      "Successfully installed lxml-4.9.2 python_docx-0.8.11\n"
     ]
    }
   ],
   "source": [
    "!pip install python_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947b11e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "#Sagemaker Endpoint Deploy\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'shibing624/text2vec-base-chinese',\n",
    "\t'HF_TASK':'feature-extraction'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\ttransformers_version='4.17.0',\n",
    "\tpytorch_version='1.10.2',\n",
    "\tpy_version='py38',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tendpoint_name='huggingface-inference-text2vec-base-chinese-v1',\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\t# instance_type='ml.m5.xlarge' # ec2 instance type\n",
    "\tinstance_type='ml.p3.2xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "633841bc-df6d-4b02-a45c-161fe65f377b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.187071323394775\n"
     ]
    }
   ],
   "source": [
    "# Inference testing\n",
    "import time\n",
    "\n",
    "hfp = sagemaker.huggingface.model.HuggingFacePredictor('huggingface-inference-text2vec-base-chinese-v1')\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(10):\n",
    "    hfp.predict({'inputs':''.join(['打印' for _ in range(100)])})[0][0][0]\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c23b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Preprocess Data\n",
    "import os\n",
    "import docx\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import json\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "def is_all_black(s):\n",
    "    for si in s:\n",
    "        if si != ' ':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def read_doc(path):\n",
    "    title = get_title(path)\n",
    "    titles = []\n",
    "    paragraphs = []\n",
    "    sentences = []\n",
    "    paragraphs_id = []\n",
    "    sentences_id = []\n",
    "    \n",
    "    document = Document(path)  # 读入文件\n",
    "    for i in range(len(document.paragraphs)):\n",
    "        p0 = document.paragraphs[i].text\n",
    "        p = document.paragraphs[i].text.replace('. ', '。')\n",
    "        if p != '':\n",
    "            ss = p.split('。')\n",
    "            for j in range(len(ss)):\n",
    "                if ss[j] != '' and is_all_black(ss[j])==False:\n",
    "                    titles.append(title)\n",
    "                    paragraphs.append(p0)\n",
    "                    sentences.append(ss[j])\n",
    "                    paragraphs_id.append(i)\n",
    "                    sentences_id.append(j)\n",
    "    df = pd.DataFrame({'title':titles, 'paragraph':paragraphs, 'sentence':sentences,\n",
    "                      'paragraph_id':paragraphs_id, 'sentence_id':sentences_id})          \n",
    "    return df\n",
    "\n",
    "def get_title(path):\n",
    "    try:\n",
    "        title = os.path.split(os.path.splitext(path)[0])[1].replace('——', '-').split('-')[1]\n",
    "    except:\n",
    "        title = os.path.split(os.path.splitext(path)[0])[1]\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b16f410c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hfp = sagemaker.huggingface.model.HuggingFacePredictor('huggingface-inference-text2vec-base-chinese-v1')\n",
    "\n",
    "def get_vector(q):\n",
    "    if len(q) > 400:\n",
    "        return [-1000 for _ in range(768)]\n",
    "    return hfp.predict({'inputs':[q]})[0][0][0]\n",
    "\n",
    "def embbeding(df):\n",
    "    df['title_vector'] = ''\n",
    "    df['sentence_vector'] = ''\n",
    "    title_vector = str(get_vector(df.iloc[0, 0]))\n",
    "    for i in range(len(df)):\n",
    "        df.iloc[i, 5] = title_vector\n",
    "        df.iloc[i, 6] = str(get_vector(df.iloc[i, 2]))\n",
    "        print('\\r embbeding %i out of %i finished'%(i, len(df)), end='')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32458c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import data to OpenSearch\n",
    "import boto3\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "host = '' # cluster endpoint, for example: my-test-domain.us-east-1.es.amazonaws.com/\n",
    "region = '' # e.g. cn-north-1\n",
    "index_name = \"\"\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "\n",
    "\n",
    "awsauth = (username, password)\n",
    "\n",
    "\n",
    "url = host+'_bulk'\n",
    "\n",
    "headers = { \"Content-Type\": \"application/json\" }\n",
    "\n",
    "def import_data(df, id_start=0, before_import=0):\n",
    "    payloads = ''\n",
    "    for i in range(id_start, len(df)+id_start):\n",
    "        first = json.dumps({ \"index\": { \"_index\": index_name, \"_id\": str(i+before_import) } }, ensure_ascii=False) + \"\\n\"\n",
    "        second = json.dumps({\"title\": str(df.iloc[i-id_start, 0]), \n",
    "                     \"paragraph\": str(df.iloc[i-id_start, 1]), \n",
    "                     \"sentence\": str(df.iloc[i-id_start, 2]), \n",
    "                     \"paragraph_id\": str(df.iloc[i-id_start, 3]), \n",
    "                     \"sentence_id\": str(df.iloc[i-id_start, 4]), \n",
    "                     \"title_vector\": json.loads(df.iloc[i-id_start, 5]),\n",
    "                     \"sentence_vector\": json.loads(df.iloc[i-id_start, 6])},\n",
    "                   ensure_ascii=False) + \"\\n\"\n",
    "        payloads += first + second\n",
    "    # print(payloads)\n",
    "    r = requests.post(url, auth=awsauth, headers=headers, data=payloads.encode()) # requests.get, post, and delete have similar syntax\n",
    "#     print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f412d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 9\u001b[0m names \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m before_import \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(names)):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "#Preprocess Data and Import\n",
    "\n",
    "hfp = sagemaker.huggingface.model.HuggingFacePredictor('huggingface-inference-text2vec-base-chinese-v1-gpu')\n",
    "\n",
    "folder_path = ''\n",
    "\n",
    "slice = 10\n",
    "\n",
    "names = os.listdir(folder_path)\n",
    "before_import = 0\n",
    "for j in range(len(names)):\n",
    "    name = names[j]\n",
    "    df = read_doc(os.path.join(folder_path, name))\n",
    "    df = embbeding(df)\n",
    "    for i in range(len(df)//slice+1):\n",
    "        import_data(df[slice*i:slice*(i+1)], slice*i, before_import)\n",
    "        print('\\r import %i out of %i finished'%(i, len(df)//slice+1), end='')\n",
    "    before_import += len(df)\n",
    "    print(' file %i out of %i finished'%(j, len(names)//slice+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adaa310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "#Create Dynamo DB\n",
    "client = boto3.client('dynamodb', region_name='us-west-2')\n",
    "\n",
    "try:\n",
    "    resp = client.create_table(\n",
    "        TableName=\"FeedbackRecordsSEWCFAQ\",\n",
    "        # Declare your Primary Key in the KeySchema argument\n",
    "        KeySchema=[\n",
    "            {\n",
    "                \"AttributeName\": \"SearchInputs\",\n",
    "                \"KeyType\": \"HASH\"\n",
    "            },\n",
    "            {\n",
    "                \"AttributeName\": \"_id\",\n",
    "                \"KeyType\": \"RANGE\"\n",
    "            }\n",
    "        ],\n",
    "        # Any attributes used in KeySchema or Indexes must be declared in AttributeDefinitions\n",
    "        AttributeDefinitions=[\n",
    "            {\n",
    "                \"AttributeName\": \"SearchInputs\",\n",
    "                \"AttributeType\": \"S\"\n",
    "            },\n",
    "            {\n",
    "                \"AttributeName\": \"_id\",\n",
    "                \"AttributeType\": \"S\"\n",
    "            }\n",
    "        ],\n",
    "        # ProvisionedThroughput controls the amount of data you can read or write to DynamoDB per second.\n",
    "        # You can control read and write capacity independently.\n",
    "        ProvisionedThroughput={\n",
    "            \"ReadCapacityUnits\": 50,\n",
    "            \"WriteCapacityUnits\": 50\n",
    "        }\n",
    "    )\n",
    "    print(\"Table created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error creating table:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc59b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
