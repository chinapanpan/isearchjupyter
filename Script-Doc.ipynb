{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b72e52e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.28.9)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.31.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.9->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.9->boto3) (1.26.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.9->boto3) (1.16.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.173.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.28.9)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.22.3)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.0.1)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.5.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.31.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.9->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting python_docx\n",
      "  Using cached python_docx-0.8.11-py3-none-any.whl\n",
      "Collecting lxml>=2.3.2 (from python_docx)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/01/ae/ce23856fb6065f254101c1df381050b13adf26088dd554a15776615d470f/lxml-4.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.0 MB)\n",
      "Installing collected packages: lxml, python_docx\n",
      "Successfully installed lxml-4.9.3 python_docx-0.8.11\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting langchain\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/59/d0/074f7fbd7323623cca4175e0323c2cff565d5cf8c6b58f5dc81f046aa29f/langchain-0.0.240-py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.0.12)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/30/85/df2259c0bee64b8c38772c43e849a7c312183d7415934546577614885170/dataclasses_json-0.5.13-py3-none-any.whl (26 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c4/ab/3dca3982889b21a132cad3d54d705db2e7dd5d33cec7f149678166641bab/langsmith-0.0.14-py3-none-any.whl (29 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/8f/34/3c2fd908056c042704c72e41396352289ccb03d72608633874530a989009/numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (1.22.3)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/a8/e7/22abb5a10733bf8142984201aedf27d4a58f5810ebdfe9679f9876c7bf4d/openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting pydantic<2,>=1 (from langchain)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b6/8e/7dd215f91528487535e7aa048e4092c20ecd0168df958e58809e2235cece/pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.29.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\n",
      "Installing collected packages: typing-inspect, pydantic, numexpr, openapi-schema-pydantic, marshmallow, langsmith, dataclasses-json, langchain\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.7.3\n",
      "    Uninstalling numexpr-2.7.3:\n",
      "      Successfully uninstalled numexpr-2.7.3\n",
      "Successfully installed dataclasses-json-0.5.13 langchain-0.0.240 langsmith-0.0.14 marshmallow-3.20.1 numexpr-2.8.4 openapi-schema-pydantic-1.2.4 pydantic-1.10.11 typing-inspect-0.9.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting pypdf\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/f0/85/4b93fed57763cd685b6da4cb43b03e3abe2208c3bb12d6775039f27a4019/pypdf-3.13.0-py3-none-any.whl (256 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.13.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting docx2txt\n",
      "  Using cached docx2txt-0.8-py3-none-any.whl\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "!pip install --upgrade boto3 -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "!pip install --upgrade sagemaker -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "!pip install python_docx -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "!pip install langchain -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "!pip install pypdf -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "!pip install docx2txt -i https://pypi.tuna.tsinghua.edu.cn/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca88125",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from chinese_text_splitter import ChineseTextSplitter\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import requests\n",
    "\n",
    "nltk.download('punkt')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ab46c74-4ef9-48b0-b4ec-8289e59ae840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from chinese_text_splitter import ChineseTextSplitter\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c462bf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2c7e860b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please put data in this folder--> docs/\n"
     ]
    }
   ],
   "source": [
    "# The name of index\n",
    "#sm_client = boto3.client('secretsmanager')\n",
    "#index_name = sm_client.get_secret_value(SecretId='opensearch-index-name')['SecretString']\n",
    "#data= json.loads(index_name)\n",
    "#index_name = data.get('index')\n",
    "#print('pre-defined index name in deployment/cdk.json-->',index_name)\n",
    "index_name = 'test'\n",
    "\n",
    "# Language, 'chinese' or 'english'\n",
    "language = 'chinese'\n",
    "\n",
    "# The name of embbeding model endpoint, usually you can keep it as default\n",
    "eb_endpoint = 'huggingface-inference-eb'\n",
    "\n",
    "# Ebbeding vector dimension, usually you can keep it as default\n",
    "v_dimension = 768\n",
    "\n",
    "# Docs file folder to be processed and ingested\n",
    "folder_path = 'docs/'\n",
    "print('Please put data in this folder-->',folder_path)\n",
    "\n",
    "# Paragraph size / Chunck size\n",
    "chunck_size = 200\n",
    "\n",
    "# The imported data of the same index_name, usually you can keep it as 0 if you are creating a new index\n",
    "before_import = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c82381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "hfp = sagemaker.huggingface.model.HuggingFacePredictor(eb_endpoint)\n",
    "\n",
    "#===================Function Definition=================\n",
    "\n",
    "def load_file(filepath,language):\n",
    "    \n",
    "    if filepath.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(filepath)\n",
    "    elif filepath.lower().endswith(\".docx\"):\n",
    "        loader = Docx2txtLoader(filepath)\n",
    "    elif filepath.lower().endswith(\".pptx\"):\n",
    "        loader = UnstructuredPowerPointLoader(filepath)\n",
    "    elif filepath.lower().endswith(\".csv\"):\n",
    "        loader = CSVLoader(filepath)\n",
    "    elif filepath.lower().endswith(\".txt\"):\n",
    "        loader = TextLoader(filepath)\n",
    "    else:\n",
    "        loader = TextLoader(filepath)\n",
    "\n",
    "    if language == \"chinese\":\n",
    "        textsplitter = ChineseTextSplitter()\n",
    "    elif language == \"english\":\n",
    "        textsplitter = NLTKTextSplitter(chunk_size=chunck_size, chunk_overlap=10)\n",
    "\n",
    "    docs = loader.load_and_split(textsplitter)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_title(path):\n",
    "    try:\n",
    "        title = os.path.split(os.path.splitext(path)[0])[1].replace('——', '-').split('-')[1]\n",
    "    except:\n",
    "        title = os.path.split(os.path.splitext(path)[0])[1]\n",
    "    return title\n",
    "\n",
    "def read_doc(path, chunck_size = chunck_size):\n",
    "    doc = load_file(path, language)\n",
    "    title = get_title(path)\n",
    "    titles = []\n",
    "    paragraphs = []\n",
    "    sentences = []\n",
    "    para = ''\n",
    "    con = 0\n",
    "    for d in doc:\n",
    "#         print('*********')\n",
    "        con += 1\n",
    "        titles.append(title)\n",
    "        sentences.append(d.page_content)\n",
    "        para += d.page_content\n",
    "        \n",
    "        if len(para) >= chunck_size:\n",
    "            \n",
    "            paragraphs += [para for _ in range(con)]\n",
    "            para = ''\n",
    "            con = 0\n",
    "    paragraphs += [para for _ in range(con)]\n",
    "    print(len(titles), len(sentences),len(paragraphs))\n",
    "    df = pd.DataFrame({'title':titles, 'paragraph':paragraphs, 'sentence':sentences})\n",
    "    return df\n",
    "\n",
    "def get_vector(q):\n",
    "    try:\n",
    "        result=hfp.predict({'inputs':[q]})\n",
    "        if \"sentence_embeddings\" in result:                                   \n",
    "            return result[\"sentence_embeddings\"][0][0]\n",
    "        else:\n",
    "            return result[0][0][0]\n",
    "        \n",
    "        #vector = hfp.predict({'inputs':[q]})[0][0][0]\n",
    "        return vector\n",
    "    except:\n",
    "        return [-1000 for _ in range(v_dimension)]\n",
    "    return hfp.predict({'inputs':[q]})[0][0][0]\n",
    "\n",
    "def embbeding(df):\n",
    "    df['title_vector'] = ''\n",
    "    df['sentence_vector'] = ''\n",
    "    title_vector = str(get_vector(df.iloc[0, 0]))\n",
    "    for i in range(len(df)):\n",
    "#         df.iloc[i, 5] = title_vector\n",
    "        df.iloc[i, 3] = str(get_vector(df.iloc[i, 2]))\n",
    "        print('\\r embbeding %i out of %i finished'%(i, len(df)), end='')\n",
    "    return df\n",
    "\n",
    "# ==============OpenSearch Related=====================\n",
    "# retrieve secret manager value by key using boto3\n",
    "sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-host-url')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "es_host_name = data.get('host')\n",
    "host = es_host_name+'/' if es_host_name[-1] != '/' else es_host_name# cluster endpoint, for example: my-test-domain.us-east-1.es.amazonaws.com/\n",
    "region = boto3.Session().region_name # e.g. cn-north-1\n",
    "# sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-master-user')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "username = data.get('username')\n",
    "password = data.get('password')\n",
    "# service = 'es'\n",
    "# credentials = boto3.Session().get_credentials()\n",
    "awsauth = (username, password)\n",
    "url = host+'_bulk'\n",
    "headers = { \"Content-Type\": \"application/json\" }\n",
    "\n",
    "payloads = {\n",
    "\"settings\": { \"index\": {\n",
    "\"knn\": True,\n",
    "\"knn.algo_param.ef_search\": 100 }\n",
    "}, \"mappings\": {\n",
    "\"properties\": { \n",
    "  \"title_vector\": {\n",
    "\"type\": \"knn_vector\", \"dimension\": v_dimension, \"method\": {\n",
    "\"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": {\n",
    "\"ef_construction\": 256,\n",
    "\"m\": 128 }\n",
    "} },\n",
    "\"sentence_vector\": {\n",
    "\"type\": \"knn_vector\", \"dimension\": v_dimension, \"method\": {\n",
    "\"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": {\n",
    "\"ef_construction\": 256,\n",
    "\"m\": 128 }\n",
    "} },\n",
    "\"title\": { \"type\": \"text\"}, \n",
    "\"sentence\": {\"type\": \"text\" }, \n",
    "\"paragraph\": {\"type\": \"text\" }, \n",
    "\"sentence_id\": {\"type\": \"text\" }, \n",
    "\"paragraph_id\": {\"type\": \"text\" }\n",
    "} }\n",
    "}\n",
    "\n",
    "# Create Index\n",
    "r = requests.delete(host+index_name, auth=awsauth, headers=headers, json={})\n",
    "r = requests.put(host+index_name, auth=awsauth, headers=headers, json=payloads)\n",
    "\n",
    "def import_data(df, id_start=0, before_import=0):\n",
    "    payloads = ''\n",
    "    for i in range(id_start, len(df)+id_start):\n",
    "        first = json.dumps({ \"index\": { \"_index\": index_name, \"_id\": str(i+before_import) } }, ensure_ascii=False) + \"\\n\"\n",
    "        second = json.dumps({\"title\": str(df.iloc[i-id_start, 0]), \n",
    "                     \"paragraph\": str(df.iloc[i-id_start, 1]), \n",
    "                     \"sentence\": str(df.iloc[i-id_start, 2]), \n",
    "                     \"sentence_vector\": json.loads(df.iloc[i-id_start, 3])},\n",
    "                   ensure_ascii=False) + \"\\n\"\n",
    "        payloads += first + second\n",
    "    # print(payloads)\n",
    "    r = requests.post(url, auth=awsauth, headers=headers, data=payloads.encode()) # requests.get, post, and delete have similar syntax\n",
    "#     print(r.text)\n",
    "\n",
    "#==============Main Preprocess Data and Import===============\n",
    "\n",
    "slice = 10\n",
    "names = os.listdir(folder_path)\n",
    "# before_import = 0\n",
    "failed_files = []\n",
    "for j in range(len(names)):\n",
    "    name = names[j]\n",
    "#     if os.path.splitext(name)[1] not in ['.doc','.docx']:continue\n",
    "    try:\n",
    "        df = read_doc(os.path.join(folder_path, name))\n",
    "        df = embbeding(df)\n",
    "        for i in range(len(df)//slice+1):\n",
    "            import_data(df[slice*i:slice*(i+1)], slice*i, before_import)\n",
    "            print('\\r import %i out of %i finished'%(i+1, len(df)//slice+1), end='')\n",
    "        before_import += len(df)\n",
    "        print(' file %i out of %i finished'%(j+1, len(names)//slice+1))\n",
    "    except Exception as ex:\n",
    "        # traceback.print_exc(file=sys.stdout)\n",
    "        failed_files.append(name)\n",
    "        print(f\"=================Exception================={ex}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "839ee97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>sentence</th>\n",
       "      <th>title_vector</th>\n",
       "      <th>sentence_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>1\\nKang X, et al. Gut 2023;0:1–11. doi:10.11...</td>\n",
       "      <td>1\\nKang X, et al. Gut 2023;0:1–11. doi:10.11...</td>\n",
       "      <td>[-1000, -1000, -1000, -1000, -1000, -1000, -10...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>2 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>2 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>[-1000, -1000, -1000, -1000, -1000, -1000, -10...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>3 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>3 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>[-1000, -1000, -1000, -1000, -1000, -1000, -10...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>4 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>4 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>[0.6970707774162292, 0.2875113785266876, 0.145...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>5 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>5 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>[0.734359860420227, -0.026049844920635223, 0.0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023</td>\n",
       "      <td>6 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>6 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>[-1000, -1000, -1000, -1000, -1000, -1000, -10...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023</td>\n",
       "      <td>7 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>7 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>[-1000, -1000, -1000, -1000, -1000, -1000, -10...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023</td>\n",
       "      <td>8 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>8 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>[-1000, -1000, -1000, -1000, -1000, -1000, -10...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023</td>\n",
       "      <td>9 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>9 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...</td>\n",
       "      <td>[-1000, -1000, -1000, -1000, -1000, -1000, -10...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023</td>\n",
       "      <td>10 Kang X, et al. Gut 2023;0:1–11. doi:10.1136...</td>\n",
       "      <td>10 Kang X, et al. Gut 2023;0:1–11. doi:10.1136...</td>\n",
       "      <td>[-1000, -1000, -1000, -1000, -1000, -1000, -10...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023</td>\n",
       "      <td>11 Kang X, et al. Gut 2023;0:1–11. doi:10.1136...</td>\n",
       "      <td>11 Kang X, et al. Gut 2023;0:1–11. doi:10.1136...</td>\n",
       "      <td>[-1000, -1000, -1000, -1000, -1000, -1000, -10...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title                                          paragraph   \n",
       "0   2023    1\\nKang X, et al. Gut 2023;0:1–11. doi:10.11...  \\\n",
       "1   2023  2 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "2   2023  3 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "3   2023  4 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "4   2023  5 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "5   2023  6 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "6   2023  7 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "7   2023  8 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "8   2023  9 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "9   2023  10 Kang X, et al. Gut 2023;0:1–11. doi:10.1136...   \n",
       "10  2023  11 Kang X, et al. Gut 2023;0:1–11. doi:10.1136...   \n",
       "\n",
       "                                             sentence   \n",
       "0     1\\nKang X, et al. Gut 2023;0:1–11. doi:10.11...  \\\n",
       "1   2 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "2   3 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "3   4 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "4   5 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "5   6 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "6   7 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "7   8 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "8   9 Kang X, et al. Gut 2023;0:1–11. doi:10.1136/...   \n",
       "9   10 Kang X, et al. Gut 2023;0:1–11. doi:10.1136...   \n",
       "10  11 Kang X, et al. Gut 2023;0:1–11. doi:10.1136...   \n",
       "\n",
       "                                         title_vector sentence_vector  \n",
       "0   [-1000, -1000, -1000, -1000, -1000, -1000, -10...                  \n",
       "1   [-1000, -1000, -1000, -1000, -1000, -1000, -10...                  \n",
       "2   [-1000, -1000, -1000, -1000, -1000, -1000, -10...                  \n",
       "3   [0.6970707774162292, 0.2875113785266876, 0.145...                  \n",
       "4   [0.734359860420227, -0.026049844920635223, 0.0...                  \n",
       "5   [-1000, -1000, -1000, -1000, -1000, -1000, -10...                  \n",
       "6   [-1000, -1000, -1000, -1000, -1000, -1000, -10...                  \n",
       "7   [-1000, -1000, -1000, -1000, -1000, -1000, -10...                  \n",
       "8   [-1000, -1000, -1000, -1000, -1000, -1000, -10...                  \n",
       "9   [-1000, -1000, -1000, -1000, -1000, -1000, -10...                  \n",
       "10  [-1000, -1000, -1000, -1000, -1000, -1000, -10...                  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
